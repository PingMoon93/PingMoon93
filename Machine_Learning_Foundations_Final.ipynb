{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1YnejRCaaZCxd5JI4gXnPDOzVYoI-rfJk",
      "authorship_tag": "ABX9TyMXyuYFcqUYfXDho+6SFs0Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PingMoon93/PingMoon93/blob/main/IS41070_Machine_Learning_Foundations_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK 1 DATA UNDERSTANDING"
      ],
      "metadata": {
        "id": "aGbR6mA_tKOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Download the dataset.\n",
        "\n",
        "2.Upload the dataset."
      ],
      "metadata": {
        "id": "Z6ty81NXtS2A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w16PFkDBs2yx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Machine Learning/IS41070-MACHINE LEARNING FOUNDATIONS/92.csv')"
      ],
      "metadata": {
        "id": "3HGY6Egrt3Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Exploration of the data."
      ],
      "metadata": {
        "id": "7LTxFMnduSU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "cmVldb2tuZmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get a quick outview of the dataset.\n",
        "data.info()"
      ],
      "metadata": {
        "id": "OlmecCTMulDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the exact number of each category\n",
        "data['category'].unique()\n",
        "categories = data['category'].unique()\n",
        "caterories_count = data['category'].value_counts()\n",
        "print(caterories_count)"
      ],
      "metadata": {
        "id": "Ds7zJFqzuwim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for the missing value\n",
        "missing_value = data.isnull().sum()\n",
        "missing_value"
      ],
      "metadata": {
        "id": "Q1VXkPOEvHm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate the wordcloud to have a visualization of the two categories\n",
        "from wordcloud import WordCloud\n",
        "science_text =' '.join(data[data['category'] == 'SCIENCE' ]['short_description'].fillna('').astype(str))\n",
        "wordcloud = WordCloud(width=800, height=800, background_color='black').generate(science_text)\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.title(f'Word Cloud for SCIENCE description')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rbKiOrLqvLVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wellness_text =' '.join(data[data['category'] == 'WELLNESS' ]['short_description'].fillna('').astype(str))\n",
        "wordcloud = WordCloud(width=800, height=800, background_color='black').generate(wellness_text)\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.title(f'Word Cloud for WELLNESS description')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q3iWXWxQxRAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word frequency analysis\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import re"
      ],
      "metadata": {
        "id": "T3WO-0dMHtXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For brevity, we'll wrap preprocessing and term frequency analysis into functions\n",
        "#Remove non-word, delete single standalone\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)  #\n",
        "    return text.strip()\n",
        "#tokenize and stop words, count frequency\n",
        "def calculate_word_frequencies(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    word_freq = Counter(filtered_tokens)\n",
        "    return word_freq\n",
        "#print most 10 common words\n",
        "def plot_word_frequencies(word_freq):\n",
        "    common_words = word_freq.most_common(10)\n",
        "    words, counts = zip(*common_words)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(words, counts)\n",
        "    plt.xlabel('Words')\n",
        "    plt.ylabel('Frequencies')\n",
        "    plt.title('Top 10 Word Frequencies')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3VXwz8jTIhwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category = 'WELLNESS'\n",
        "wellness_descriptions = data[data['category'] == category]['short_description'].fillna('').astype(str)\n",
        "#join the text\n",
        "wellness_text = ' '.join(wellness_descriptions)\n",
        "# preprocess the text and calculate the frequency\n",
        "cleaned_text = preprocess_text(wellness_text)\n",
        "word_freq = calculate_word_frequencies(cleaned_text)\n",
        "plot_word_frequencies(word_freq)"
      ],
      "metadata": {
        "id": "fT49JeZ9IzAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category = 'SCIENCE'\n",
        "science_descriptions = data[data['category'] == category]['short_description'].fillna('').astype(str)\n",
        "#join the text\n",
        "wellness_text = ' '.join(science_descriptions)\n",
        "\n",
        "# preprocess the text and calculate the frequency\n",
        "cleaned_text = preprocess_text(science_text)\n",
        "word_freq = calculate_word_frequencies(cleaned_text)\n",
        "plot_word_frequencies(word_freq)"
      ],
      "metadata": {
        "id": "1KVLEngjJodC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate statistics about the length of different categories of short description text\n",
        "text_lengths_wellness = data[data['category'] == 'WELLNESS']['short_description'].apply(lambda x: len(str(x)))\n",
        "text_lengths_wellness_mean = text_lengths_wellness.mean()\n",
        "text_lengths_wellness_median = text_lengths_wellness.median()\n",
        "text_lengths_science = data[data['category'] == 'SCIENCE']['short_description'].apply(lambda x: len(str(x)))\n",
        "text_lengths_science_mean = text_lengths_science.mean()\n",
        "text_lengths_science_median = text_lengths_science.median()\n",
        "print(f\"The mean length of short descriptions in the WELLNESS category is: {text_lengths_wellness_mean}\")\n",
        "print(f\"The median length of short descriptions in the WELLNESS category is: {text_lengths_wellness_median}\")\n",
        "print(f\"The mean length of short descriptions in the SCIENCE category is: {text_lengths_science_mean}\")\n",
        "print(f\"The median length of short descriptions in the SCIENCE category is: {text_lengths_science_median}\")\n"
      ],
      "metadata": {
        "id": "tdvMiWMuxsCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate statistics about the length of different categories of headline\n",
        "headline_lengths_wellness = data[data['category'] == 'WELLNESS']['headline'].apply(lambda x: len(str(x)))\n",
        "headline_lengths_wellness_mean = headline_lengths_wellness.mean()\n",
        "headline_lengths_wellness_median = headline_lengths_wellness.median()\n",
        "headline_lengths_science = data[data['category'] == 'SCIENCE']['headline'].apply(lambda x: len(str(x)))\n",
        "headline_lengths_science_mean = headline_lengths_science.mean()\n",
        "headline_lengths_science_median = headline_lengths_science.median()\n",
        "print(f\"The mean length of headline in the WELLNESS category is: {headline_lengths_wellness_mean}\")\n",
        "print(f\"The median length of headline in the WELLNESS category is: {headline_lengths_wellness_median}\")\n",
        "print(f\"The mean length of headline in the SCIENCE category is: {headline_lengths_science_mean}\")\n",
        "print(f\"The median length of headline in the SCIENCE category is: {headline_lengths_science_median}\")\n"
      ],
      "metadata": {
        "id": "mXzvZHQu1W77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there are 5984 articles in the wellness category, 1,996 articles in the science category, and 20 articles with no category information.In addition, categories are unevenly distributed, with wellness samples much higher than science, and subsequent training of the model needs to focus on this.\n",
        "\n",
        "The word cloud and word frequencies of the short descriptions reflect the characteristics of the two categories to a certain extent. The high-frequency words of the health category include life, people, health and so on, which are clearly related to health. The high-frequency words scientists, space, earth, etc. of the science category are also obviously related to scientific topics.\n",
        "\n",
        "As for the length of the sentences, there is a clear difference across categories, clearly the average sentence length of wellness short description is significantly longer than that of science category, as is the median.As for the length of the title, it is the fact that the average title length and median of the Science category are higher than those of the wellness category.\n"
      ],
      "metadata": {
        "id": "eVSf3Qy5JzZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK2  Data Preparation & Modelling"
      ],
      "metadata": {
        "id": "P8zhLzswKP6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Splitting the dataset"
      ],
      "metadata": {
        "id": "oyiDmye-Ls9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This task focuses on building a model for classifying text\n",
        "#so we first remove the data with missing categories\n",
        "data_cleaned = data.dropna(subset=['category'])\n",
        "data_cleaned.info()"
      ],
      "metadata": {
        "id": "PYap8RpbLqsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#The dataset is divided into a training set and a temporary set at a ratio of 6:4\n",
        "#And then the temporary set is divided into a test set and a validation set at a ratio of 1:1\n",
        "train_data,temp_data = train_test_split(data_cleaned,test_size=0.4, random_state=42)\n",
        "test_data, valid_data = train_test_split(temp_data,test_size=0.5,random_state=42)"
      ],
      "metadata": {
        "id": "iNGzgSaaL_oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The split datasets were saved separately as csv files and saved in google drive\n",
        "train_file_path = '/content/drive/MyDrive/Machine Learning/IS41070-MACHINE LEARNING FOUNDATIONS/train data.csv'\n",
        "test_file_path = '/content/drive/MyDrive/Machine Learning/IS41070-MACHINE LEARNING FOUNDATIONS/test data.csv'\n",
        "valid_file_path = '/content/drive/MyDrive/Machine Learning/IS41070-MACHINE LEARNING FOUNDATIONS/valid data.csv'"
      ],
      "metadata": {
        "id": "BV3QyLA8MHY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the data to csv\n",
        "train_data.to_csv(train_file_path, index=False)\n",
        "test_data.to_csv(test_file_path, index=False)\n",
        "valid_data.to_csv(valid_file_path, index=False)"
      ],
      "metadata": {
        "id": "bnPbma5NMbP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Load the train and valid data and apply preprocess steps."
      ],
      "metadata": {
        "id": "4lgexQcvMn4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/Machine Learning/IS41070-MACHINE LEARNING FOUNDATIONS/train data.csv')\n",
        "valid_data = pd.read_csv('/content/drive/MyDrive/Machine Learning/IS41070-MACHINE LEARNING FOUNDATIONS/valid data.csv')"
      ],
      "metadata": {
        "id": "X4q_ubOdMhfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove irrelevant information\n",
        "train_data = train_data.drop(columns=['link','authors','date'])\n",
        "valid_data = valid_data.drop(columns=['link','authors','date'])"
      ],
      "metadata": {
        "id": "1bYxt1C2yY6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.info()"
      ],
      "metadata": {
        "id": "A-vC2VyjNJ-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_data.info()"
      ],
      "metadata": {
        "id": "jB-tbs5dNM_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "# define preprocess function and name it as clean_text\n",
        "def clean_text(text):\n",
        "    '''\n",
        "    Perform stop-words removal and lemmatization\n",
        "    '''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z?.!,¿]+|http\\S+\", \" \", text)\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    words = [word for word in text.split() if word not in stopwords.words('english')]\n",
        "    words = [WordNetLemmatizer().lemmatize(word) for word in words]\n",
        "    return \" \".join(words)"
      ],
      "metadata": {
        "id": "elVNDUQqNUl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.2Compare whether there are differences in modeling before and after preprocessing"
      ],
      "metadata": {
        "id": "WSPXpFOLN1Ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "Giyy5u_DPA31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the fillna () function to fill the missing values with empty strings\n",
        "valid_data['short_description'].fillna('', inplace=True)\n",
        "train_data['short_description'].fillna('', inplace=True)"
      ],
      "metadata": {
        "id": "gIrlEPnZNmHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unpreprocess\n",
        "x_train1 = train_data['short_description'].astype(str)\n",
        "y_train1 = train_data['category']\n",
        "x_valid1 = valid_data['short_description'].astype(str)\n",
        "y_valid1 = valid_data['category']\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "x_train_tf1 = vectorizer.fit_transform(x_train1)\n",
        "x_valid_tf1 = vectorizer.transform(x_valid1)"
      ],
      "metadata": {
        "id": "rzsLkjq6OagJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = LogisticRegression()"
      ],
      "metadata": {
        "id": "XOZ6oXp5PR1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "model1.fit(x_train_tf1,y_train1)"
      ],
      "metadata": {
        "id": "1M1qlSkMPW63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred1 = model1.predict(x_valid_tf1)"
      ],
      "metadata": {
        "id": "r7R2PdpKPe4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_valid1,y_pred1))"
      ],
      "metadata": {
        "id": "aPafVkcTPn-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "ccMn5NMmTnwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#With preprocess steps\n",
        "train_data['short_description_pre'] = train_data['short_description'].apply(clean_text)\n",
        "valid_data['short_description_pre'] = valid_data['short_description'].apply(clean_text)"
      ],
      "metadata": {
        "id": "upOlQaUEP8p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "vfDzMX_AQPo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train2 = train_data['short_description_pre'].astype(str)\n",
        "y_train2 = train_data['category']\n",
        "x_valid2 = valid_data['short_description_pre'].astype(str)\n",
        "y_valid2 = valid_data['category']\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "x_train_tf2 = vectorizer.fit_transform(x_train2)\n",
        "x_valid_tf2 = vectorizer.transform(x_valid2)"
      ],
      "metadata": {
        "id": "aN2Xvoy6Qblg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = LogisticRegression()\n",
        "model2.fit(x_train_tf2,y_train2)"
      ],
      "metadata": {
        "id": "2YB-8Ag0QyQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred2 = model2.predict(x_valid_tf2)\n",
        "print(classification_report(y_valid2,y_pred2))"
      ],
      "metadata": {
        "id": "4dDQ5pwLQ6NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above analysis, Model1 has better overall performance in the two categories, especially in the recall rate and F1-score of the SCIENCE category, which can be considered as a better model choice. Without text processing, Model 1 draws conclusions that contradict our general belief that text preprocessing is beneficial for model tuning."
      ],
      "metadata": {
        "id": "BBC2rXcpRljX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Build binary classification models\n",
        "\n",
        "Having trained the logistic regression model in Task 5, let's use the same method to train the Random Forest model."
      ],
      "metadata": {
        "id": "uEvhLc21SF_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build RandomForrest model with unpreprocess text\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_model1 = RandomForestClassifier()"
      ],
      "metadata": {
        "id": "BG6ki0Y6SNfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the RandomForrest model\n",
        "rf_model1.fit(x_train_tf1,y_train1)"
      ],
      "metadata": {
        "id": "s0WGPshYS9oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred1_rf = rf_model1.predict(x_valid_tf1)\n",
        "print(classification_report(y_valid1,y_pred1_rf))"
      ],
      "metadata": {
        "id": "v0Zvgg_gTQb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build another RandomForrest Model using preprocess text\n",
        "rf_model2 = RandomForestClassifier()\n",
        "rf_model2.fit(x_train_tf2,y_train2)"
      ],
      "metadata": {
        "id": "rEWC89kyTj6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred2_rf = rf_model2.predict(x_valid_tf2)\n",
        "print(classification_report(y_valid2,y_pred2_rf))"
      ],
      "metadata": {
        "id": "dlX7l9cBTzQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrating the evaluation metrics of the two models, rf_model2 performs better in the recall and overall macro average metrics of the SCIENCE category, while it is slightly better than rf_model1 in the precision and F1-score of the WELLNESS category. Therefore, it can be argued that rf_model2 is the better choice of the two models, especially when dealing with the SCIENCE samples. rf_model2 uses the preprocessed text for training, indicating that text preprocessing has a positive effect on the tuning of the random forest model."
      ],
      "metadata": {
        "id": "1FUQOJTeUnNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Build an end-to-end classifier using deep learning"
      ],
      "metadata": {
        "id": "hhmf2OwMVcDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using PyTorch and train the new model from scratch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re"
      ],
      "metadata": {
        "id": "9KEuUpVjVlQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_data = pd.read_csv('/content/drive/MyDrive/Machine Learning/IS41070-MACHINE LEARNING FOUNDATIONS/92.csv')\n",
        "#drop the missing value\n",
        "news_data.dropna(subset=['short_description','category'],inplace=True)"
      ],
      "metadata": {
        "id": "595F-bMuYltK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_data.info()"
      ],
      "metadata": {
        "id": "P4hD7wOHY47C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text preprocess\n",
        "news_data['short_description_pre'] = news_data['short_description'].apply(clean_text)\n",
        "#Encoder\n",
        "label_encoder = LabelEncoder()\n",
        "news_data['category'] = label_encoder.fit_transform(news_data['category'])\n",
        "#Split the data\n",
        "x_train, x_test, y_train, y_test = train_test_split(news_data['short_description_pre'], news_data['category'], test_size=0.2, random_state=42)\n",
        "#Convert text data to TF-IDF feature vectors\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "x_train_tfidf = vectorizer.fit_transform(x_train).toarray()\n",
        "x_test_tfidf = vectorizer.transform(x_test).toarray()"
      ],
      "metadata": {
        "id": "iU372F1hY7Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Constructor to initialize the dataset object\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = torch.tensor(texts, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "B5EHBH2uae2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converts pandas objects to list\n",
        "y_train = y_train.tolist()\n",
        "y_test = y_test.tolist()\n",
        "train_dataset = TextDataset(x_train_tfidf, y_train)\n",
        "test_dataset = TextDataset(x_test_tfidf, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "HMVtrZpRbCKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the module\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Set model parameters\n",
        "input_dim = 5000  # Number of features after TF-IDF vectorization\n",
        "hidden_dim = 100\n",
        "output_dim = len(label_encoder.classes_)\n",
        "\n",
        "# Build the model\n",
        "model = TextClassifier(input_dim, hidden_dim, output_dim)"
      ],
      "metadata": {
        "id": "ewph3dj0bFTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define criterion and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for texts, labels in train_loader:\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "model.eval()\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "with torch.no_grad():\n",
        "    for texts, labels in test_loader:\n",
        "        outputs = model(texts)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "correct = sum(p == l for p, l in zip(all_predictions, all_labels))\n",
        "total = len(all_labels)\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy on test set: {accuracy:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(all_labels, all_predictions))"
      ],
      "metadata": {
        "id": "PkM5Gyi5ckRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above results show that the accuracy of the model on the test set is 86.44%, which means that 86.44% of the samples in the test data set are correctly classified by the model."
      ],
      "metadata": {
        "id": "1hsum_IulBWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK3 Evaluation"
      ],
      "metadata": {
        "id": "2kIdP-Ssc29p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Choose a primary metric to evaluate the models"
      ],
      "metadata": {
        "id": "BXPj2SqZdGNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train five models: two logistic regression models (model1 and model2), where model1 is not text preprocessed and model2 is text preprocessed; There are also two random forest models (rf_model1 and rf_model2), where rf_model1 has no text preprocessing and rf_model2 has text preprocessing. The last model is a deep learning model trained using PyTorch.\n",
        "\n",
        "We used classification reports to evaluate model performance. We mainly considered F1-score and accuracy as evaluation metrics, because accuracy intuitively reflects the overall prediction accuracy of the model, and F1-score combines precision and recall. More reliable when dealing with class imbalance (Kampakis, 2023).\n"
      ],
      "metadata": {
        "id": "ABxJqEiYgUp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Evaluate the performance of each model."
      ],
      "metadata": {
        "id": "vAXP-sKIwFr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Comparing Model1 with Model2, which uses short description after text preprocessing for model training, Model1 performs significantly better than Model2 in the SCIENCE category, especially in the recall rate and F1-score. The recall rate of Model2 in the SCIENCE category is only 0.33, which may lead to missing many samples in the SCIENCE category. There is little difference between the precision of Model1 and Model2 in the WELLNESS category, but Model2 has a slightly higher recall, Model1 has a slightly higher F1-score in the WELLNESS category, and the overall performance is slightly better than Model2. In the overall evaluation, the weighted average metrics (precision, recall, and F1-score) of Model1 are better than those of Model2, indicating its overall better performance.Without text processing, Model 1 has a better performance, which draws conclusions that contradict our general belief that text preprocessing is beneficial for model training.\n",
        "\n",
        "2.Comparing rf_model1 and rf_model2, the latter uses the short description after text preprocessing for model training, the precision of rf_model1 is slightly higher than that of rf_model2 in the SCIENCE category, but the recall rate is lower. As a result, the F1-score is also lower. The recall and F1-score of rf_model2 in the SCIENCE category are slightly better than those of rf_model1. Besides, the precision and F1-score of rf_model2 in the WELLNESS category are slightly higher than those of rf_model1. In particular, it has an advantage in recall. Integrating the evaluation metrics of the two models, rf_model2 performs better in the recall and overall macro average metrics of the SCIENCE category, while it is slightly better than rf_model1 in the precision and F1-score of the WELLNESS category. Therefore, it can be argued that rf_model2 is the better choice of the two models, especially when dealing with the SCIENCE samples.\n",
        "\n",
        "3.Comparing model2 with rf_model2, the former is a logistic regression model, and the latter is a random forest model. In the SCIENCE category, the Precision of Model2 is 0.90, the Recall is 0.55, and the F1-score is 0.68. rf_model2's Precision is 0.94, Recall is 0.47, and F1-score is 0.63. Although rf_model2 is slightly higher in precision, Model2 performs better in recall and F1-score. This indicates that Model2 is more balanced and comprehensive in identifying the SCIENCE category. For the wellness category, the Precision of Model2 is 0.87, the Recall is 0.98, the F1-score is 0.92, the Precision of rf_model2 is 0.85, the Recall is 0.99, With an F1-score of 0.91, both models perform similarly on this category, but slightly higher on the Model2 metric. From the analysis of comprehensive performance indicators, Model2 is more balanced and comprehensive in processing data, especially in the case of class imbalance, its performance in each category is relatively better,which draws the conclusion that in this analysis, category 'headline' seems to be a better feature than 'short_description'.\n",
        "\n",
        "In conclusion, among the four models trained, rf_model2 with short description after preprocessing has the best comprehensive performance."
      ],
      "metadata": {
        "id": "eUDW8p-xtt_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.Error analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "y9Y5VnAcSHfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing model classification reports, except model 2, the accuracy of our trained models reached more than 85%. Through the analysis of classification reports, we found that the performance of WELLNESS and SCIENCE was different, and the precision and recall of all models in Wellness category were higher than those in SCIENCE category. In addition, The recall rate of the SCIENCE category is not high, and the reason for the above phenomenon is mainly the problem of class imbalance in the data set. The data of WELLNESS is 5984, and the data of SCIENCE is 1996, the former is almost 3 times that of the latter. In addition to this reason, the choice of feature value is too single is another reason. The model is trained with a single feature value, and can be trained with multiple feature values later.\n",
        "\n",
        "Let's take a look at the part of the random forest and logistic regression model that predict wrong together, here we choose model2 and rf_model2 as a comparison:"
      ],
      "metadata": {
        "id": "P8fPYfGf20DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "incorrect_indices_model2 = set(np.where(y_valid2 != y_pred2)[0])\n",
        "incorrect_indices_rf_model2 = set(np.where(y_valid2 != y_pred2_rf)[0])\n",
        "\n",
        "common_incorrect_indices = incorrect_indices_model2.intersection(incorrect_indices_rf_model2)\n",
        "for index in common_incorrect_indices:\n",
        "    print(f\"Index: {index}, True Label: {y_valid2.iloc[index]}, Model 2 Predicted Label: {y_pred2[index]}, rf_model2 Predicted Label: {y_pred2_rf[index]}\")\n",
        "    print(f\"Text: {x_valid2.iloc[index]}\")\n",
        "    print('-' * 80)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZTCWqdq8uSpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of common incorrect predictions: {len(common_incorrect_indices)}\")"
      ],
      "metadata": {
        "id": "HjhIHy4ivonY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count the number of classification errors\n",
        "error_counts = {}\n",
        "\n",
        "for index in common_incorrect_indices:\n",
        "    true_label = y_valid2.iloc[index]\n",
        "    pred_label2 = y_pred2[index]\n",
        "    pred_label3 = y_pred2_rf[index]\n",
        "\n",
        "    # Error in model 2\n",
        "    if true_label != pred_label2:\n",
        "        if (true_label, pred_label2) not in error_counts:\n",
        "            error_counts[(true_label, pred_label2)] = 0\n",
        "        error_counts[(true_label, pred_label2)] += 1\n",
        "\n",
        "    # Error in rf_model2\n",
        "    if true_label != pred_label3:\n",
        "        if (true_label, pred_label3) not in error_counts:\n",
        "            error_counts[(true_label, pred_label3)] = 0\n",
        "        error_counts[(true_label, pred_label3)] += 1\n",
        "\n",
        "# Print the number of classification errors\n",
        "print(\"Error counts:\")\n",
        "for (true_label, pred_label), count in error_counts.items():\n",
        "    print(f\"True Label: {true_label}, Predicted Label: {pred_label}, Count: {count}\")\n"
      ],
      "metadata": {
        "id": "Om3tKhjzyn6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above results show that both models make a total of 129 errors, and the number of misclassified SCIENCE as WELLNESS is 242, far more than the number of misclassified WELLNESS as SCIENCE is 16, indicating that when the model distinguishes between WELLNESS and SCIENCE, The better recognition ability for the WELLNESS category may be related to the fact that there are more samples in the WELLNESS category in the sample.\n",
        "\n",
        "By analyzing the parts of the text that were incorrectly Predicted by both models, we found that the misprediction may also result from the presence of high-frequency words in another category in these texts, for example, \"Index: 600, True Label: SCIENCE, Model 2 Predicted Label: WELLNESS, rf_model2 Predicted Label: WELLNESS\n",
        "Text:  one important part pet owner looking pet health well \",both models predicted the original SCIENCE to be WELLNESS. health appears in this text, which is a high-frequency word in the WELLNESS category.\n",
        "\n",
        "From this analysis, I learned how to find the samples where the two models jointly predicted wrong, and analyze these samples. By looking at the wrong samples, I can identify the possible classification problems of the model, such as the high-frequency vocabulary problem and the data imbalance problem mentioned above. Understanding the possible reasons for the error can help improve the model. Different feature values, such as headline, can be selected to improve the model in subsequent research."
      ],
      "metadata": {
        "id": "x5uvOAZjxZNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.Model optimization\n",
        "\n",
        "We have already analyzed the influence of preprocess, let's use headline as the factor to train the model."
      ],
      "metadata": {
        "id": "b8Ezy49bXU4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use headline as factor and build a new logic model.\n",
        "#Use the fillna() function to fill the missing values.\n",
        "train_data['headline'].fillna('',inplace=True)\n",
        "valid_data['headline'].fillna('',inplace=True)"
      ],
      "metadata": {
        "id": "O-iz79GYwCnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess the headline\n",
        "train_data['headline_pre']=train_data['headline'].apply(clean_text)\n",
        "valid_data['headline_pre']=valid_data['headline'].apply(clean_text)"
      ],
      "metadata": {
        "id": "85907DO4xqvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "tqYwOzA_Um_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train3= train_data['headline_pre'].astype(str)\n",
        "y_train3= train_data['category']\n",
        "x_valid3= valid_data['headline_pre'].astype(str)\n",
        "y_valid3= valid_data['category']\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "x_train_tf3 = vectorizer.fit_transform(x_train3)\n",
        "x_valid_tf3 = vectorizer.transform(x_valid3)\n",
        "model3 = LogisticRegression()\n",
        "model3.fit(x_train_tf3,y_train3)"
      ],
      "metadata": {
        "id": "9jYxR2C-yHqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred3 = model3.predict(x_valid_tf3)\n",
        "print(classification_report(y_valid3,y_pred3))"
      ],
      "metadata": {
        "id": "jRNVyRuUy6FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Comparing Model 2 and Model 3, Model 3 performs slightly better than Model 2, especially in the precision and recall of the SCIENCE category, and for the WELLNESS category the two models are very close, and model 3 has a slight advantage in recall. In terms of dealing with class imbalance, Model 3 is better than Model 2 in all indicators, indicating that choosing headline as feature value and using logistic regression model is better than choosing short description."
      ],
      "metadata": {
        "id": "gfkdUvYKz-UW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use headline as factor to train a Random Forrest model\n",
        "rf_model3 = RandomForestClassifier()\n",
        "rf_model3.fit(x_train_tf3,y_train3)"
      ],
      "metadata": {
        "id": "Dd6WuqcD09H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred3_rf = rf_model3.predict(x_valid_tf3)\n",
        "print(classification_report(y_valid3,y_pred3_rf))"
      ],
      "metadata": {
        "id": "O2HHEf0T1qN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Comparing rf_model2 and rf_model3, the former uses short description, and the latter uses headline as feature value, it is found that rf_model3 has improved. It outperforms rf_model2 on most metrics, especially in handling class imbalance and improving recall for the SCIENCE category.\n",
        "\n",
        "According to the above conclusions, the headline is selected as the feature value, and the model ability is optimized, and the accuracy of the optimized model has reached to 90%. Next, let's optimize the deep learning model."
      ],
      "metadata": {
        "id": "B-oeGEik2F5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_data1 = pd.read_csv('/content/drive/MyDrive/Machine Learning/IS41070-MACHINE LEARNING FOUNDATIONS/92.csv')"
      ],
      "metadata": {
        "id": "jpdmAgudVJCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_data1.dropna(subset=['headline','category'],inplace=True)"
      ],
      "metadata": {
        "id": "iHyfF6WYPZIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_data1.info()"
      ],
      "metadata": {
        "id": "Wu8z-ZveVF94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text preprocess\n",
        "news_data1['headline_pre'] = news_data1['headline'].apply(clean_text)\n",
        "#Encoder\n",
        "label_encoder = LabelEncoder()\n",
        "news_data1['category'] = label_encoder.fit_transform(news_data1['category'])\n",
        "#Split the data\n",
        "x_train1, x_test1, y_train1, y_test1 = train_test_split(news_data1['headline_pre'], news_data1['category'], test_size=0.2, random_state=42)\n",
        "#Convert text data to TF-IDF feature vectors\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "x_train_tfidf1 = vectorizer.fit_transform(x_train1).toarray()\n",
        "x_test_tfidf1 = vectorizer.transform(x_test1).toarray()"
      ],
      "metadata": {
        "id": "KPqX-SKDPWuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_data1.info()"
      ],
      "metadata": {
        "id": "XXymnKSwWJCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.3)  # Change Dropout layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Apply dropout after activation\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Assuming you have defined train_loader and test_loader properly\n",
        "\n",
        "# Set model parameters\n",
        "input_dim = 5000  # Number of features after TF-IDF vectorization\n",
        "hidden_dim = 100\n",
        "output_dim = len(label_encoder.classes_)\n",
        "\n",
        "# Build the model\n",
        "model = TextClassifier(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Define criterion and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 5  # Reduce epochs to 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for texts, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate on test set\n",
        "model.eval()\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "with torch.no_grad():\n",
        "    for texts, labels in test_loader:\n",
        "        outputs = model(texts)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "correct = sum(p == l for p, l in zip(all_predictions, all_labels))\n",
        "total = len(all_labels)\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy on test set: {accuracy:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(all_labels, all_predictions))"
      ],
      "metadata": {
        "id": "KwnAvGMZfXo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Choosing headline as the feature value and decreasing epoch to 5 and increasing dropput to 0.3 show a slight improvement in model performance. Compared with the first trained deep learning model, the Accuracy of the optimized model (0.87 vs 0.86) is improved.\n",
        "The Precision, Recall and F1-Score of the optimized models Macro Avg and Weighted Avg are better than those before optimization, showing better balance and comprehensive performance."
      ],
      "metadata": {
        "id": "WtvAv_1AbOL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.Merge train_data and valid_data, and perform cross validation."
      ],
      "metadata": {
        "id": "Jz3vwpEdlzpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/Machine Learning/IS41070-MACHINE LEARNING FOUNDATIONS/train data.csv')\n",
        "valid_data = pd.read_csv('/content/drive/MyDrive/Machine Learning/IS41070-MACHINE LEARNING FOUNDATIONS/valid data.csv')"
      ],
      "metadata": {
        "id": "H1yDgT9mp02x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merge the train data and valid data and name it as combined data\n",
        "combined_data = pd.concat([train_data,valid_data], ignore_index=True)"
      ],
      "metadata": {
        "id": "g2zA3YqRqAL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data.info()"
      ],
      "metadata": {
        "id": "EH_R_4nFqN-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data.dropna(subset=['headline','category'],inplace=True)"
      ],
      "metadata": {
        "id": "qQxj0J_ItNL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score,KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "x = combined_data['headline'].astype(str)\n",
        "y = combined_data['category']\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "x_tfidf = vectorizer.fit_transform(x).toarray()\n",
        "model3 = LogisticRegression()\n",
        "rf_model3 = RandomForestClassifier()"
      ],
      "metadata": {
        "id": "kNQPbQG9qXX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of folds for cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation and evaluate model performance\n",
        "cv_scores_model3 = cross_val_score(model3, x_tfidf, y, cv=kf, scoring='accuracy')\n",
        "cv_scores_rf_model3 = cross_val_score(rf_model3, x_tfidf, y, cv=kf, scoring='accuracy')\n",
        "\n",
        "# Print cross-validation results\n",
        "print(\"Logistic Regression Cross-validation scores:\", cv_scores_model3)\n",
        "print(\"Logistic Regression Mean CV accuracy:\", cv_scores_model3.mean())\n",
        "\n",
        "print(\"Random Forest Cross-validation scores:\", cv_scores_rf_model3)\n",
        "print(\"Random Forest Mean CV accuracy:\", cv_scores_rf_model3.mean())"
      ],
      "metadata": {
        "id": "JvonEXccrX_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The above results show that both models exhibit relatively high cross-validation accuracy, indicating that they can learn and make accurate predictions on the dataset, where the random forest model has a higher average accuracy than the logistic regression model, indicating that the trained random forest model is slightly better than the logistic regression model."
      ],
      "metadata": {
        "id": "KR0TIHgCvziN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform cross validation of PyTorch model\n",
        "#(How to Use K-fold Cross Validation With DataLoaders in PyTorch | Saturn Cloud Blog, 2024)\n",
        "# Convert text data to TF-IDF features\n",
        "from sklearn.metrics import accuracy_score\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "x_tfidf = vectorizer.fit_transform(x).toarray()\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Define model hyperparameters\n",
        "input_dim = 5000\n",
        "hidden_dim = 100\n",
        "output_dim = len(label_encoder.classes_)\n",
        "\n",
        "# Define KFold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Prepare to store cross-validation results\n",
        "cv_scores = []\n",
        "\n",
        "# Define training and evaluation function\n",
        "def train_and_evaluate(train_index, val_index):\n",
        "    # Create the model\n",
        "    model = TextClassifier(input_dim, hidden_dim, output_dim)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Get training and validation data\n",
        "    x_train, x_val = x_tfidf[train_index], x_tfidf[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    # Convert to tensors\n",
        "    x_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "    x_val = torch.tensor(x_val, dtype=torch.float32)\n",
        "    y_val = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "    # Train the model\n",
        "    num_epochs = 5\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validate the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x_val)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        accuracy = accuracy_score(y_val.numpy(), predicted.numpy())\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Perform cross-validation\n",
        "for train_index, val_index in kf.split(x_tfidf):\n",
        "    accuracy = train_and_evaluate(train_index, val_index)\n",
        "    cv_scores.append(accuracy)\n",
        "\n",
        "# Print cross-validation results\n",
        "print(\"PyTorch Cross-validation scores:\", cv_scores)\n",
        "print(\"PyTorch Mean CV accuracy:\", sum(cv_scores) / len(cv_scores))\n"
      ],
      "metadata": {
        "id": "zx9A2fgnzYFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The above results show the variability of the scores, indicating that the performance of the model is not consistent on different data splits, especially the third fold, which exhibits a very low accuracy. The average accuracy is 0.6971, which is not a good performance."
      ],
      "metadata": {
        "id": "Rajd2phv0YJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.Select the best model from previous and apply to test data.From previous items the best moder is rf_model2"
      ],
      "metadata": {
        "id": "aDiP_27C7aFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load test data\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Machine Learning/IS41070-MACHINE LEARNING FOUNDATIONS/test data.csv')\n",
        "test_data = test_data.drop(columns=['authors','link','date'])"
      ],
      "metadata": {
        "id": "L4X1AHlXN6pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head()"
      ],
      "metadata": {
        "id": "D6xJ__Z3N_P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess the test data\n",
        "test_data['short_description'].fillna('',inplace=True)\n",
        "test_data['short_description_pre']=test_data['short_description'].apply(clean_text)"
      ],
      "metadata": {
        "id": "Hy8y3YTxOCvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "vwRmpZUJorb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['short_description'].fillna('',inplace=True)\n",
        "train_data['short_description_pre'] = train_data['short_description'].apply(clean_text)"
      ],
      "metadata": {
        "id": "7d65-uwnncLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train2 = train_data['short_description_pre'].astype(str)\n",
        "y_train2 = train_data['category']\n",
        "x_test2 = test_data['short_description_pre'].astype(str)\n",
        "y_test2 = test_data['category']\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "x_train_tf2 = vectorizer.fit_transform(x_train2)\n",
        "x_test_tf2 = vectorizer.transform(x_test2)\n",
        "\n",
        "#apply test data to rf_model2\n",
        "rf_model2 = RandomForestClassifier()\n",
        "rf_model2.fit(x_train_tf2,y_train2)\n",
        "y_pred_test2 = rf_model2.predict(x_test_tf2)\n",
        "print(classification_report(y_test2,y_pred_test2))\n"
      ],
      "metadata": {
        "id": "Fjet_tkBOPSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above results show the results of applying rf_model2 with test data. It is found that the model accuracy does not improve but slightly decreases with test data. In addition, the classification of the WELLNESS category was improved using test data."
      ],
      "metadata": {
        "id": "0UeYtRF1pTsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.Choose the model with best performance,retrain it and apply to test_data.From 11 we know rf_model3 is the model with best performance."
      ],
      "metadata": {
        "id": "MNMvEGfa5Ai7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load test data\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Machine Learning/IS41070-MACHINE LEARNING FOUNDATIONS/test data.csv')"
      ],
      "metadata": {
        "id": "6jy0FZnGy2ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head()"
      ],
      "metadata": {
        "id": "0XIvoDUazFgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess the test data\n",
        "test_data['headline'].fillna('',inplace=True)\n",
        "test_data['headline_pre']=test_data['headline'].apply(clean_text)"
      ],
      "metadata": {
        "id": "FI9SV6h-z5CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data.info()"
      ],
      "metadata": {
        "id": "i37KHMT_B_ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess\n",
        "combined_data['headline_pre'] = combined_data['headline'].apply(clean_text)"
      ],
      "metadata": {
        "id": "Qq6F6lB6B0K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data.head()"
      ],
      "metadata": {
        "id": "y-y3fz6tDnjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data.info()"
      ],
      "metadata": {
        "id": "J0aLS36uEgnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Retrain the model with more data, that is combined_data\n",
        "x_train_combined= combined_data['headline_pre'].astype(str)\n",
        "y_train_combined= combined_data['category']\n",
        "x_test= test_data['headline_pre'].astype(str)\n",
        "y_test= test_data['category']\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "x_train_tf_combined = vectorizer.fit_transform(x_train_combined)\n",
        "x_test_tf1= vectorizer.transform(x_test)\n",
        "#Retrain the rf_model3 using combined data\n",
        "rf_model4 = RandomForestClassifier()\n",
        "rf_model4.fit(x_train_tf_combined,y_train_combined)"
      ],
      "metadata": {
        "id": "M8RwDzEp08qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test_rf = rf_model4.predict(x_test_tf1)\n",
        "print(classification_report(y_test,y_pred_test_rf))"
      ],
      "metadata": {
        "id": "bmKNCj0z2wGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Compared with rf_model3 in 11,rf_model4 uses more data to train the model, but it does not improve. On the contrary, some indicators decline (accuracy from 0.90 to 0.89), which may mean that the current amount of data is large enough. Further increasing the amount of data did not bring significant performance gains."
      ],
      "metadata": {
        "id": "GCRoF9s5-hRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References\n",
        "\n",
        "How to Use k-fold Cross Validation with DataLoaders in PyTorch | Saturn Cloud Blog. (2024, January 14). https://saturncloud.io/blog/how-to-use-kfold-cross-validation-with-dataloaders-in-pytorch/\n",
        "\n",
        "Kampakis, S. (2023, December 9). What is the F-1 measure and why is it useful for imbalanced class problems? The Data Scientist. https://thedatascientist.com/f-1-measure-useful-imbalanced-class-problems/\n"
      ],
      "metadata": {
        "id": "UT8GXxxmjm5A"
      }
    }
  ]
}
